{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7bcb5e0-9a66-4fcc-97d2-608c4502a220",
   "metadata": {},
   "source": [
    "## Estimating class probabilities\n",
    "\n",
    "this involves estimating the probability that an instance belongs to a particular class k. \n",
    "\n",
    "to do so, the decision tree traverses the thress to find the leaf node for the instance, then it returns the ration of training instance to class k in the node.\n",
    "\n",
    "example:\n",
    "\n",
    "- suppose you have found a flower whose petals are 5 cm long and 1.5 cm wide. \n",
    "- The corresponding leaf node is the depth-2 left node, so the Decision Tree should output the following probabilities: 0% for Iris setosa (0/54), 90.7% for Iris versicolor (49/54), and 9.3% for Iris virginica (5/54).\n",
    "- And if you ask it to predict the class, it should output Iris versicolor (class 1) because it has the highest probability.\n",
    " eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9f1677d-dd20-48d4-a5bb-90b8cf05d467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.90740741 0.09259259]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "## estimating class probabilities\n",
    "#====================================================================\n",
    "\n",
    "print(tree_clf.predict_proba([[5,1.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac5d962-ddd5-443f-9cbf-27efa1c29d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(tree_clf.predict([[5,1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50052ba7-0b32-44a0-a505-3d07fd0a2041",
   "metadata": {},
   "source": [
    "## The CART Training algorithm\n",
    "\n",
    "Scikit-learn uses the classification and regresssion Tree(CART) algorithm to train decision trees. \n",
    "\n",
    "How it works:\n",
    "\n",
    "1. it works by first spliting the training set into two subsets using a single feature *k* and a threshold  *tk*\n",
    "2. to choose *k* and *tk* it searches for the pair (*k, tk*) that produces the purest subsets(weighted by there size).\n",
    "3. Once the CART algorithm has successful split the training set in two, it splits the subset using the same logic, then the sub-subsets, and so on recusively.\n",
    "4. it stops recursing once it reaches the maximum depth i.e `max_depth` parameter.\n",
    "\n",
    "\n",
    "## Gini Impurity or Entropy?\n",
    "\n",
    "By default the gini impurity method is used but one can select the `entropy impurity measure instead by setting `creterion` hyper-parameter to \"entropy\"/\n",
    "\n",
    "`gini impurity` measures are slightly faster to compute making it a good default. \n",
    "\n",
    "However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a38ab5-5562-4459-bf88-6d34601fa789",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters\n",
    "\n",
    "Decision trees ake very few assumptions about the training data. If left unconstrained, the tree structure will adopt itself to the training data and might result to overfiting. Such a model is called *nonparametric model* because the number of parameters is not determined prior to training. \n",
    "\n",
    "- on the contrast, parametric models such as linear models have predetermined number of parameters,\n",
    "\n",
    "to prevent overfitting, one needs to **restrict the maximum depth of the decision tree**. \n",
    "\n",
    "- in Scikit-Learn, this is controlled by the `max_depth` hyperparameter.\n",
    "\n",
    "Other parameters  by the DecisionTreeClassifier include:\n",
    "\n",
    "1. `min_sample_split`:- the minimum number of samples a node must have before it can be split\n",
    "2. `min_sample)leaf`:- the minimum number of samples a leaf node must have\n",
    "3. `min_weight_fraction_leaf`:- same as min_sample_leaf but expressed as a fraction\n",
    "4. `max_leaf_nodes`:- maximum number of leaf nodes\n",
    "5. `max_features`:- the maximum number of features that are evaluated for spliting each node."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
