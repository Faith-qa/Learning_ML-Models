{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4565840-d257-432f-9850-dd4001eea234",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks with Keras\n",
    "\n",
    "ANN is a machine learning model inspired bt the network of of biological neurons in the brain.\n",
    "\n",
    "ANN architectures:\n",
    "\n",
    "###  the Perceptron\n",
    "\n",
    "- :- also called the Linear threshold unit. input and output are numbers and each input connection is associated with a weight. it then commputes a weighted sum of it's inputs then applies a step function to that sum and outputs the results.\n",
    "\n",
    "- the perceptron learning rule reinforces connections that help reduce  the error. it is fed on one training instanece at a time, for eact instance it makes a prediction.\n",
    "\n",
    "- let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84271cd9-8ce4-43d2-87f2-14ef7aa95311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:, (2,3)] # petal length and petal width\n",
    "y = (iris.target == 0).astype(int)  # Iris setosa?\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "y_pred = per_clf.predict([[2,0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fdc10-c4ca-415a-bb5b-52ac90005373",
   "metadata": {},
   "source": [
    "Perceptron as a netowork was dropped mainly because of its many limitations i.e:\n",
    "\n",
    "- they are incapable of solving trivial problems\n",
    "- XOR problem i.e **exclusing OR**:- a logical operation that takes two binary input and returns true if exactly one of the inputs is true\n",
    "- because of the above,the Multilayer Perceptron and Backpropagation were invented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10a2992-875c-4972-b203-3b702fffb6ac",
   "metadata": {},
   "source": [
    "### the Multilayer Perceptron and Backpropagation\n",
    "\n",
    "this is type of **artificial neural network** that is designed to learn from data. it's called \"multilayer\" because it consists of several layers of nodes( or \"neurons\"), each performing calculations that help the network learn to make predictions or decisions.\n",
    "\n",
    "**structure of an MLP**\n",
    "\n",
    "1. input Layer:-\n",
    "* this is the first layer in the network. it takes the raw input data. each input feature is fed into a seperate neuron in this layer. For example, if you working with a dataset that has two features(like height and weight), the input layer will have 2 neuron.\n",
    "\n",
    "2. Hidden layers:-\n",
    "   \n",
    "   * they process inputs. they are the layers where the learning most happens.\n",
    "   * TLU (Threshold Logic Unit): each neuron in the hidden layer is a TLU which performs calculations and passes the output to the next layer.\n",
    "   * the layers are fully connected meaning every neuron in one layer connects to every neurons in the next layer.\n",
    "\n",
    "3. Output Layer:-\n",
    "\n",
    "    * this is the final layer that provides the prediciton of results. the number of neurons in this layer depends on the type of problem you are solving. (i.e one neuron for binary clasification or multiple neurons for multi-class classification).\n",
    "  \n",
    "4. Bias Neuron:-\n",
    "\n",
    "    * Each layer (except the output layer) includes a bias neuron which helps adjust the output independently of the input. this can improve the model's ability to fit the data.\n",
    "  \n",
    "**How the MLP Work**\n",
    "\n",
    "* Feedforward Neural Network(FNN):\n",
    "\n",
    "      * The MLP is a type of FNN where information flows in one direction-from input to output - without looping back.\n",
    "\n",
    "* Deep Neural Network (DNN):\n",
    "\n",
    "    * When an MLP has many hidden layers it's called a Deep Neural Network.  \"Deep Learning\" often refers to working with these deeper networks.\n",
    "\n",
    "*An MLP is a neural network with layers of neurons that learns by adjusting its internal parameters through a process called backpropagation. This process involves making predictions, measuring errors, and updating weights to improve accuracy over time. By repeating these steps, the MLP becomes better at understanding and predicting based on the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55962c4-fd2a-40e9-908c-8b9d557c33ce",
   "metadata": {},
   "source": [
    "#### Training an MLP with Backpropagetion\n",
    "\n",
    "this involves teaching it to make accurate predictions by adjusting it's internal parameters (weights and biases).\n",
    "\n",
    "**Steps include**:\n",
    "\n",
    "1. Forward Pass:\n",
    "   \n",
    "   * the input is fed through the network layer by layer, calculating outputs until the final prediction is made. this is like making an initial guess.\n",
    "\n",
    "2. Calculate Error:\n",
    "\n",
    "   * the network's prediction is compared tothe actual result using a loss function which measures the error or difference.\n",
    "\n",
    "3. Backward Pass (Backpropagation):\n",
    "\n",
    "    * **Error contribution**: the network determines how much each weight contributed to the error\n",
    "    * it uses the **chain rule** from calculus to calculate the gradient which shows how much each weight \n",
    " should change to reduce the error.\n",
    "this process happens in reverse, from the output layer back to the input layer.\n",
    "\n",
    "4. Gradient Descent:\n",
    "\n",
    "   * the network uses the calculated gradients to adjust the weights and biases, aiming to reduce the error. This step is repeated many times, across multiple passes through the data (epochs), to refine the predictions.\n",
    "  \n",
    "**key concepts**\n",
    "\n",
    "* Automatic Differentiation(Autodiff):\n",
    "      \n",
    "      * this is a technique to compute gradients automatically, which makes backpropagation efficient and precise.\n",
    "\n",
    "* Mini-Batch\n",
    "\n",
    "      * instead of processing the entire dataset at once, the data is divided into small batches, allowing the network to update weights more frequently and improve learning.\n",
    "\n",
    "* Epoch:\n",
    "\n",
    "    * One complete pass through the entire dataset. training usually involves multiple epochs to ensure the network learns effectively.\n",
    " \n",
    "*An MLP is a neural network with layers of neurons that learns by adjusting its internal parameters through a process called backpropagation. This process involves making predictions, measuring errors, and updating weights to improve accuracy over time. By repeating these steps, the MLP becomes better at understanding and predicting based on the input data.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-neural",
   "language": "python",
   "name": "env-neural"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
