{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8e8c0e-62a9-43df-97eb-ef8d5dd83b95",
   "metadata": {},
   "source": [
    "# ML Models\n",
    "\n",
    "## creating and using machine learning model\n",
    "\n",
    "### TFAutoModel\n",
    "this is a class in machine learning library that easily allows for the creation of models\n",
    "\n",
    "**What does TFAutoModel do**\n",
    "\n",
    "* Wrapper:- it is a \"Wrapper\" package or cover of different types of models\n",
    "* Smart guess:- can automatically figure out which model architecture(structure) to use based on a checkpoint(state of a model at a particular stage in training)\n",
    "\n",
    "**How does TFAutomodel work?**\n",
    "\n",
    "1. Automatic model selection:- when selected, it looks at the checkpoint then figures out which model to create\n",
    "2. Instantiate the model:- it then creates the appropriate model with the correct architecture\n",
    "\n",
    "let's dive into creating a model with a focus on the Bert model\n",
    "\n",
    "**Creating a Transformer model**\n",
    "\n",
    "***Step 1: loading the configuration object***\n",
    "the configuration object `config` contains the various settings and parameters that define how the Ber model should be built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f255055-068b-4f2b-8c8b-e9e9306a5ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the Bert model\n",
    "\n",
    "from transformers import BertConfig, TFBertModel\n",
    "\n",
    "#building the config\n",
    "config = BertConfig()\n",
    "\n",
    "#Building the model from the config\n",
    "model = TFBertModel(config)\n",
    "\n",
    "#Model is randomly initialized\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ffbf8-0c2f-40ff-88b9-8e5a2fc01479",
   "metadata": {},
   "source": [
    "the above initialized model can be used but the output will be meaningless because it hasn't learned anything yet. it needs to be trained.\n",
    "\n",
    "training from scratch takes time and data thereby consumes a lot of computational resources and is expensive on the environment. As a result:\n",
    "\n",
    "to save on time and resources, we use existing pre-trained models. these have learned from large datasets and can be finetuned for specific tasks making them more efficient.\n",
    "\n",
    "To load a transformer model that is already trained, we simply use the `from_pretrained()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eaff25-5472-4c12-bfec-6f3daa882a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "\n",
    "model = TFBertModel.from_pretrained(\"bert-base-cased\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
