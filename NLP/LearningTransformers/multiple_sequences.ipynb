{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4511ec-a74c-468f-8642-c3396b03e9f7",
   "metadata": {},
   "source": [
    "## Handling multiple sequences\n",
    "\n",
    "- transformer models expect multiple sequences/sentences by default\n",
    "- for example:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289e89cd-3ba6-40e7-b1ec-8a13f135df7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tf.Tensor(\n",
      "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
      "   2166  1012]], shape=(1, 14), dtype=int32)\n",
      "Logits: tf.Tensor([[-2.7276196  2.8789363]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = tf.constant([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f77bb-2830-450f-9c06-1e6be02b9a5b",
   "metadata": {},
   "source": [
    "Let's explain what is happening in the code above:\n",
    "\n",
    "tensorflow:- is an ml library often used for building and training neural networks. in the context of the above code it is being usef to manage data as tensors which are mulyi-dimensional arrays.\n",
    "\n",
    "AutoTokenizer and TFAutoModelForSequenceClassification are classes from the transformers library by Hugging Face, which provides tools for using pre-trained language models.\n",
    "\n",
    "the above code uses a pre-trained DistilBERT model to classify the sentiment of a given sentence. It converts the sentence into a format the model understands, runs the sentence through the model, and outputs the raw prediction scores (logits). These scores can then be further processed to determine the predicted sentiment of the input text.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
