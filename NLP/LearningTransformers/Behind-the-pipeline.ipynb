{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c92448-ef07-45c7-8a50-81ff90bf2385",
   "metadata": {},
   "source": [
    "## Behind the pipeline\n",
    "\n",
    "There are 3 stages in the pipeline:\n",
    "\n",
    "* Tokenizer\n",
    "* Model\n",
    "* PostProcessing\n",
    "\n",
    "\n",
    "*Process*\n",
    "\n",
    "**Raw Text** ====> **Numbers[Input IDs]** ====> **Outputs [Logits]** ===>\n",
    "**Predictions**\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "The process has several steps i.e: \n",
    "\n",
    "1. the text is split into small chunks called tokens\n",
    "2. it will add special tokens\n",
    "3. matches each token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea692b9-6297-466a-b032-82d489cc2bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.17.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./venv/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./venv/lib/python3.12/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from tensorflow) (70.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.4.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in ./venv/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in ./venv/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.17.0-cp312-cp312-macosx_12_0_arm64.whl (236.3 MB)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.17.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "absl-py==2.1.0\n",
      "anyio==4.4.0\n",
      "appnope==0.1.4\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asttokens==2.4.1\n",
      "astunparse==1.6.3\n",
      "async-lru==2.0.4\n",
      "attrs==23.2.0\n",
      "Babel==2.15.0\n",
      "beautifulsoup4==4.12.3\n",
      "bleach==6.1.0\n",
      "certifi==2024.7.4\n",
      "cffi==1.16.0\n",
      "charset-normalizer==3.3.2\n",
      "comm==0.2.2\n",
      "debugpy==1.8.2\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "executing==2.0.1\n",
      "fastjsonschema==2.20.0\n",
      "filelock==3.15.4\n",
      "flatbuffers==24.3.25\n",
      "fqdn==1.5.1\n",
      "fsspec==2024.6.1\n",
      "gast==0.6.0\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.64.1\n",
      "h11==0.14.0\n",
      "h5py==3.11.0\n",
      "httpcore==1.0.5\n",
      "httpx==0.27.0\n",
      "huggingface-hub==0.23.4\n",
      "idna==3.7\n",
      "ipykernel==6.29.5\n",
      "ipython==8.26.0\n",
      "ipywidgets==8.1.3\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.4\n",
      "json5==0.9.25\n",
      "jsonpointer==3.0.0\n",
      "jsonschema==4.23.0\n",
      "jsonschema-specifications==2023.12.1\n",
      "jupyter==1.0.0\n",
      "jupyter-console==6.6.3\n",
      "jupyter-events==0.10.0\n",
      "jupyter-lsp==2.2.5\n",
      "jupyter_client==8.6.2\n",
      "jupyter_core==5.7.2\n",
      "jupyter_server==2.14.2\n",
      "jupyter_server_terminals==0.5.3\n",
      "jupyterlab==4.2.3\n",
      "jupyterlab_pygments==0.3.0\n",
      "jupyterlab_server==2.27.2\n",
      "jupyterlab_widgets==3.0.11\n",
      "keras==3.4.1\n",
      "libclang==18.1.1\n",
      "Markdown==3.6\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe==2.1.5\n",
      "matplotlib-inline==0.1.7\n",
      "mdurl==0.1.2\n",
      "mistune==3.0.2\n",
      "ml-dtypes==0.4.0\n",
      "namex==0.0.8\n",
      "nbclient==0.10.0\n",
      "nbconvert==7.16.4\n",
      "nbformat==5.10.4\n",
      "nest-asyncio==1.6.0\n",
      "notebook==7.2.1\n",
      "notebook_shim==0.2.4\n",
      "numpy==1.26.4\n",
      "opt-einsum==3.3.0\n",
      "optree==0.12.1\n",
      "overrides==7.7.0\n",
      "packaging==24.1\n",
      "pandocfilters==1.5.1\n",
      "parso==0.8.4\n",
      "pexpect==4.9.0\n",
      "platformdirs==4.2.2\n",
      "prometheus_client==0.20.0\n",
      "prompt_toolkit==3.0.47\n",
      "protobuf==4.25.3\n",
      "psutil==6.0.0\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pycparser==2.22\n",
      "Pygments==2.18.0\n",
      "python-dateutil==2.9.0.post0\n",
      "python-json-logger==2.0.7\n",
      "PyYAML==6.0.1\n",
      "pyzmq==26.0.3\n",
      "qtconsole==5.5.2\n",
      "QtPy==2.4.1\n",
      "referencing==0.35.1\n",
      "regex==2024.5.15\n",
      "requests==2.32.3\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rich==13.7.1\n",
      "rpds-py==0.19.0\n",
      "safetensors==0.4.3\n",
      "Send2Trash==1.8.3\n",
      "setuptools==70.3.0\n",
      "six==1.16.0\n",
      "sniffio==1.3.1\n",
      "soupsieve==2.5\n",
      "stack-data==0.6.3\n",
      "tensorboard==2.17.0\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorflow==2.17.0\n",
      "termcolor==2.4.0\n",
      "terminado==0.18.1\n",
      "tinycss2==1.3.0\n",
      "tokenizers==0.19.1\n",
      "tornado==6.4.1\n",
      "tqdm==4.66.4\n",
      "traitlets==5.14.3\n",
      "transformers==4.42.4\n",
      "types-python-dateutil==2.9.0.20240316\n",
      "typing_extensions==4.12.2\n",
      "uri-template==1.3.0\n",
      "urllib3==2.2.2\n",
      "wcwidth==0.2.13\n",
      "webcolors==24.6.0\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.8.0\n",
      "Werkzeug==3.0.3\n",
      "wheel==0.43.0\n",
      "widgetsnbextension==4.0.11\n",
      "wrapt==1.16.0\n"
     ]
    }
   ],
   "source": [
    "# install the required dependencies in the virtual environment venv\n",
    "\n",
    "# install tensorflow\n",
    "\n",
    "!pip install tensorflow\n",
    "!pip freeze > requirements.txt\n",
    "!cat requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02d7983-24f1-49d5-ae6c-307cfe861808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "print(\"hello\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f689f6-7a40-452b-9679-2d400a8e98d0",
   "metadata": {},
   "source": [
    "## Pipeline process explained\n",
    "\n",
    "it groups together 3 steps:\n",
    "* tokenizer\n",
    "* model\n",
    "* post processing\n",
    "\n",
    "### preprocessing with tokenizer\n",
    "\n",
    "1. test input is converted to numbers that the model can make sense of using `tokenizers` responsible for : splitting input, mapping each token to integer and adding additional inputs that may be usefull to the model\n",
    "\n",
    "2. preprocessing needs to be done in the exact same way the model was pretrained. to achieve this, the `AutoTokenizer class` and its `from_pretranined()` method is used. Using the checkpoint name of the model, it will automatically fetch the data associated with the model tokenizer and cache it such that it is only downloaded once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c2d47ca-06eb-4b34-88a0-b3314abe54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the autotokenizer class\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb2619-5012-4575-add5-5d8197509a43",
   "metadata": {},
   "source": [
    "3. once the tokenizer is created as above:- sentences can be passed. output will be a dictionary that's ready to feed to the model\n",
    "4. convert list of input id's to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09dc8e98-d1c2-4992-a34c-9a80ab5c4f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\n",
      "array([[ 101, 1045, 1005, 2310, 2042, 3403, 2005, 2023, 2607, 2026, 2878,\n",
      "        2166,  102],\n",
      "       [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102,    0,    0,    0,\n",
      "           0,    0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "raw_inputs = [\n",
    "    \"i've been waiting for this course my whole life\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3b318-28c7-4099-b3be-691013ebce62",
   "metadata": {},
   "source": [
    "**Going through the model**\n",
    "\n",
    "we can download the pretrained model the same way i.e transformers provides a `TFAutoModel` class which equally has a `from_pretrained` method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1eddf5f-26dc-4a5d-90c5-2b4821c600f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca2eb1-6546-4627-be9b-4a61ccaf4853",
   "metadata": {},
   "source": [
    "In the above code model, we have:\n",
    "\n",
    "* downloaded the same pipeline used before and instantiated a model with it\n",
    "* the above architecture contains only the base transformee module: given some inputs, its outputs are called `hidden states` also known as `features`\n",
    "* hidden states or features are usually inputs to another model often known as the head\n",
    "\n",
    "\n",
    "**A high-dimensional vector?**\n",
    "***context: beginner***\n",
    "*meaning of terms*:\n",
    "\n",
    "* Vector:- think of a list in python or array in java i.e a list of numbers\n",
    "* high-dimension vector:- a big list.\n",
    "* transformer module:- type of machine learning model used for processing sequential data like sentences in a language. it takes the data, transforms it to machine readable format.\n",
    "\n",
    "When we say, vector output by the transformer module is \"high dimensional\" we are talking about the size and complexity of the data/ vector itself. It usually has 3 dimensions:\n",
    "\n",
    "* **Batch size** :- the number of sequences processed at a time\n",
    "* **Sequence length** :- the length of the numerical representation of the sequence\n",
    "*  **Hidden size**:- the vector(inferr to the list analogy) dimension of each model input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5f2e300-c0c7-4921-b70c-e5b31aff948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 13, 768)\n"
     ]
    }
   ],
   "source": [
    "## we can see this if we feed the input we preprocessed to the model\n",
    "outputs = model(inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce353377-552d-4451-9d6d-617aa1454341",
   "metadata": {},
   "source": [
    "Do note the output huggingface transformers model behave like named tuples of dictioneries \n",
    "\n",
    "### Model Heads: Making sense out of numbers\n",
    "\n",
    "***What is a model head***\n",
    "\n",
    "this is the part of the model, mostly the transformer model that takes the final data and makes sense of it. same functionality as the brain. \n",
    "\n",
    "***functions of model heads i.e what do they do***\n",
    "\n",
    "1. it takes on high-dimensional data (think of big size python list) and projects them onto a different dimension(a direction or an axis on a graph). they do so leveraging on simple mathematical layers called linear layers.\n",
    "\n",
    "***How does this work within the transformer model***\n",
    "\n",
    "1. **Embedding Layer**:- first part of the transformer model. Takes each input word(or token) into a vector, a list of numbers that represent a word\n",
    "2. **Subsequent Layers**:- they use something called the attention mechanism to manipulate vectors and understand relationships between the words. this produces a final representation of the entire sentence.\n",
    "3. **Model Head**:- after the transformer model processes the data, the output is sent to the model head. the model head then processes this data to perform a specific task.\n",
    "\n",
    "***Different Tasks for Different Heads***\n",
    "\n",
    "There are many types of model heads, each designed for aspecific task. Here are some examples:\n",
    "\n",
    "* **Model** :- just retrieves the hidden states(vectors) without any specific task\n",
    "* **ForCasualLM** :- used for generating tEXTS\n",
    "* **ForMaskedLM**:- used for filling in missing words in a sentence\n",
    "* **ForMultipleChoice**:- used for answering multiple-choice questions\n",
    "* **ForQuestionAnswering**:- used for answering questions based on texts\n",
    "* **ForSequenceClassification**:- used for classifying entire sentences (like determining if a sentence is positive or negative).\n",
    "* **ForTokenClassification**:- used for classifying individual words in a sentence.\n",
    "\n",
    "\n",
    "For the context of our example, we will need a model with a sequence classification head (to be able to classify the sentence as positive or negative). As a result we wont use the `TFAutoModel` class, but the `TFAutoModelForSequenceClassification` as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa636e7-5941-4f0e-a6f1-c1e87bafd47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(inputs)\n",
    "\n",
    "# check the shape of the data\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0779f8e-10b5-4612-9fd8-d52d3ce251cc",
   "metadata": {},
   "source": [
    "since we had just 2 sentences and two labels, the results we get from the molde os of shape 2 x 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c24e6-b19c-438b-94a3-85776f8c64f6",
   "metadata": {},
   "source": [
    "### Postprocessing the Output\n",
    "\n",
    "Values gottent from the outputs of a model don't necessarily make sense. \n",
    "\n",
    "**Context**\n",
    "***Raw model outputs(Logits)***\n",
    "\n",
    "The raw outputs of a model are called `logits`. They are unprocessed score\n",
    "for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f930e8d-6765-4354-a78d-d11e636b74bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-2.6304553  2.6135018]\n",
      " [ 4.1692314 -3.3464472]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd1d6d-d59a-4b19-83eb-d2621085b7f9",
   "metadata": {},
   "source": [
    "The above outputs mean:\n",
    "\n",
    "* for the first sentence, the model gave a score of [-1.5607, 1.6123]\n",
    "* for the second sentence, the model gave scores of [4.1692, -3.3464]\n",
    "\n",
    "***What are logits***\n",
    "given they are the raw outputs of a model expressing performance, they are not probabilities yet. to convert them into probabilities, or something we can understand, we need to process them further.\n",
    "\n",
    "To convert logits to probabilities, we use a function called `softMax`. This function turns the raw scores into values between 0 and 1, which add up to 1 like percentages\n",
    "\n",
    "Below is the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71798d5f-d944-497a-8423-bc6c9aca1419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5.2515999e-03 9.9474841e-01]\n",
      " [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "predictions = tf.math.softmax(outputs.logits, axis=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efbe91-caab-464a-8168-42fba5867cd0",
   "metadata": {},
   "source": [
    "The above output simply means:\n",
    "\n",
    "* the model predicted [0.00525(0.525%), 0.995(99.5%)] for the first sentence and [0.9995 (99.95%), 0.0005(0.05%)] for the second one.\n",
    "\n",
    "  \"*the above attributes are converted based on the place values at the end of the output i.e 5.25...-03] means it is o.00525*\"\n",
    "\n",
    "To get the labels corresponding to each position we can inspect the id2label attribute of the model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f3b89c5-a8a9-42c8-9af5-12a625ee4f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57debeb2-72c3-41f8-93f6-fdef974d5966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
