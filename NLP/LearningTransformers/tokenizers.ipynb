{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a830c1-aae1-4f12-8c76-41a1d33a3b81",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "There objective is to translate text in Natural Language Processing to raw numbers that can be processed by the model.\n",
    "\n",
    "This step is a key commponent in NLP pipelines.\n",
    "\n",
    "There are different types of tokenizer algorithms and in this section, we will dive into a few:\n",
    "\n",
    "1. **Word-based Tokenizers**\n",
    " This is the idea of splitting raw text into words. Each word has a specific id attached to it.\n",
    "\n",
    "there are different ways to split a text i.e whitespaces can be used to tokenize text into words by applying Python's split fuction. eg:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682053e0-07bd-4ff7-8668-3a9be3b2416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'lovely', 'world']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = \"hello lovely world\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce605ea2-084d-4b45-b9f5-7dfbe53d7566",
   "metadata": {},
   "source": [
    "The downside of word based tokenizer is that it is difficult to assign id's to large vocabulary of words. For example: dog and dogs while they are related, the model will recognize them as unrelated given each set of words will be assigned different tokens.\n",
    "\n",
    "In addition, for words not in the vocabulary, the model is likely to return unkown tokens with the abbreviation \"<unk>\" or unknown. which is generally a bad sign\n",
    "\n",
    "one way to reduce the amount of unkown tokens is to go one level deaper using a *character-based tokenizer*\n",
    "\n",
    "2. **Character-based tokenizers**\n",
    "\n",
    "   \n",
    "text is split into individual characters as oppossed to words. it has 2 main benefits:\n",
    "\n",
    "* the vocanulary is much smaller\n",
    "* there are much fewer out-of-vocabulary tokens since every word can be built from characters\n",
    "\n",
    "Some concerns:\n",
    "\n",
    "* intuitively one can argue that characters are less meaningful since they don't mean much on there own.\n",
    "* we'lll likely end up with very large amount of tokens to be processed by the model\n",
    "\n",
    "\n",
    "3. **subword tokenizers**\n",
    "\n",
    "   \n",
    "relies on the principle that frequently used words should not be split into smaller subwords but rare words should be decomposed into meaningful subwords.\n",
    "\n",
    "### Loading and saving tokenizers\n",
    "\n",
    "- based on the same 2 methods i.e: `from_pretrained()` and `save_pretrained()`. these two methods will load and save the algorithm used by the tokernizer\n",
    "\n",
    "- let's chck the example with the BertTokenizer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63763f56-d33b-4e01-b344-db45ec7535e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28728eff765b405dbf19014a653857f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7e87862d624d4eba32544140b288a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393aa8774de0406087d73c89bf73c60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465c641108694ae8ae0a01a284c24ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa3a894-8df6-4a13-a87e-503ac9646c4a",
   "metadata": {},
   "source": [
    "The same can be done for the TFAutoModel, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d757450-c14e-4859-8629-ac816ed8b476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1606, 170, 11303, 1200, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# we can now use the tokenizer as was shown earlier\n",
    "\n",
    "tokenizer(\"using a transformer network is simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e15bc669-3770-4a4e-8e49-86a12a9d5f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('directory_on_my_computer/tokenizer_config.json',\n",
       " 'directory_on_my_computer/special_tokens_map.json',\n",
       " 'directory_on_my_computer/vocab.txt',\n",
       " 'directory_on_my_computer/added_tokens.json',\n",
       " 'directory_on_my_computer/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now save the tokenizer using the save_pretrained(\"directory on my computer\")\n",
    "\n",
    "tokenizer.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a076b7b3-a9a9-4f58-82de-b3aa3e3af7cf",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "***The tokenizer pipeline***\n",
    "\n",
    "Encoding is the process of translating texts to numbers. it is doen in a two step process. i.e:\n",
    "\n",
    "- the tokenization\n",
    "- conversion to input_ids\n",
    "\n",
    "step 1: tokenization:- split text to words/characters etc. because there are multiple rules that can govern the process, it is essential to instantiate a tokenizer using the name of the model inorder to make sure we use the same rules that were used when the model was pretrained.\n",
    "\n",
    "Step 2: generated tokens in step 1 are converted into numbers. this allows us to build a tensor out of them inorder to feed them to the model. to achieve this, the tokenizer has a vocabulary, which is the downloaded part when we instantiate it with the from_pretrained() method. \n",
    "\n",
    "let's test it out with some code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a4c4e9-26be-4632-b032-4e00df0ea823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"using a transformer network is simple\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b0a6e1-4e6d-436f-872a-b91ffc7267d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1606, 170, 11303, 1200, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "# to convert ti input id's we use the convert_tokens_to_ids() tokenizer method:\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c43325-bf85-416d-a7d7-aeb77e306945",
   "metadata": {},
   "source": [
    "#### Decoding\n",
    "\n",
    "this is the reverse of encoding i.e converting the id's back to a string\n",
    "\n",
    "- to achieve this we use the `decode()` method\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425fc319-550e-4c4e-b66b-98c02b6132a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decode_string = tokenizer.decode([1606, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decode_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74ce86-b829-4057-8385-89494805bc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
