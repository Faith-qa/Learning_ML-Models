{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a830c1-aae1-4f12-8c76-41a1d33a3b81",
   "metadata": {},
   "source": [
    "## Tokenizers\n",
    "\n",
    "There objective is to translate text in Natural Language Processing to raw numbers that can be processed by the model.\n",
    "\n",
    "This step is a key commponent in NLP pipelines.\n",
    "\n",
    "There are different types of tokenizer algorithms and in this section, we will dive into a few:\n",
    "\n",
    "1. **Word-based Tokenizers**\n",
    " This is the idea of splitting raw text into words. Each word has a specific id attached to it.\n",
    "\n",
    "there are different ways to split a text i.e whitespaces can be used to tokenize text into words by applying Python's split fuction. eg:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682053e0-07bd-4ff7-8668-3a9be3b2416b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'lovely', 'world']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = \"hello lovely world\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce605ea2-084d-4b45-b9f5-7dfbe53d7566",
   "metadata": {},
   "source": [
    "The downside of word based tokenizer is that it is difficult to assign id's to large vocabulary of words. For example: dog and dogs while they are related, the model will recognize them as unrelated given each set of words will be assigned different tokens.\n",
    "\n",
    "In addition, for words not in the vocabulary, the model is likely to return unkown tokens with the abbreviation \"<unk>\" or unknown. which is generally a bad sign\n",
    "\n",
    "one way to reduce the amount of unkown tokens is to go one level deaper using a *character-based tokenizer*\n",
    "\n",
    "2. **Character-based tokenizers**\n",
    "text is split into individual characters as oppossed to words. it has 2 main benefits:\n",
    "\n",
    "* the vocanulary is much smaller\n",
    "* there are much fewer out-of-vocabulary tokens since every word can be built from characters\n",
    "\n",
    "Some concerns:\n",
    "\n",
    "* intuitively one can argue that characters are less meaningful since they don't mean much on there own.\n",
    "* we'lll likely end up with very large amount of tokens to be processed by the model\n",
    "\n",
    "\n",
    "3. **subword tokenizers**\n",
    "relies on the principle that frequently used words should not be split into smaller subwords but rare words should be decomposed into meaningful subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63763f56-d33b-4e01-b344-db45ec7535e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
