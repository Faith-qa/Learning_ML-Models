{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e79bcf1-5e56-4536-a899-8669f108c23d",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c3341-7bd7-4072-9938-23369fbffc39",
   "metadata": {},
   "source": [
    "1. Why is it generally preferable to use a logistic regression classifier rather than a classic perceptron (i.e., a single layer of threshold logic unitstrained using the perceptron training algorithm)? How can you tweak a perceptron to make it equivalent to a logistic regression classifier?\n",
    "   \n",
    "   ***A classical Perceptron will converge only if the dataset is linearly separable, and it won't be able to estimate class probabilities. In contrast, a Logistic Regression classifier will generally converge to a reasonably good solution even if the dataset is not linearly separable, and it will output class probabilities. If you change the Perceptron's activation function to the sigmoid activation function (or the softmax activation function if there are multiple neurons), and if you train it using Gradient Descent (or some other optimization algorithm minimizing the cost function, typically cross entropy), then it becomes equivalent to a Logistic Regression classifier.***\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8e2f3-1c4a-4215-98bd-00427cd456ef",
   "metadata": {},
   "source": [
    "2. Why was the sigmoid activation function a key ingredient in training the first MLPs?\n",
    "   \n",
    "   ***The sigmoid activation function was a key ingredient in training the first MLPs because its derivative is always nonzero, so Gradient Descent can always roll down the slope. When the activation function is a step function, Gradient Descent cannot move, as there is no slope at all.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e158f8-fe5b-4140-ba2c-901537d1cbdf",
   "metadata": {},
   "source": [
    "3. Name three popular activation functions. Can you draw them?\n",
    "   \n",
    "   ***opular activation functions include the step function, the sigmoid function, the hyperbolic tangent (tanh) function, and the Rectified Linear Unit (ReLU) function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c740f-c78b-4de7-b6ae-726dcd18c706",
   "metadata": {},
   "source": [
    "4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "    1. What is the shape of the input matrix X?\n",
    "    2. What are the shapes of the hidden layer’s weight matrix Wh and bias vector bh?\n",
    "    3. What are the shapes of the output layer’s weight matrix Wo and bias vector bo?\n",
    "    4. What is the shape of the network’s output matrix Y?\n",
    "    5. Write the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo, and bo.\n",
    "\n",
    "   ***Considering the MLP described in the question, composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons, where all artificial neurons use the ReLU activation function:***\n",
    "\n",
    "    - ***The shape of the input matrix X is m × 10, where m represents the training batch size.***\n",
    "    - **The shape of the hidden layer's weight matrix Wh is 10 × 50, and the length of its bias vector bh is 50.***\n",
    "    - **The shape of the output layer's weight matrix Wo is 50 × 3, and the length of its bias vector bo is 3.***\n",
    "    - ***The shape of the network's output matrix Y is m × 3.***\n",
    "    - ***`Y = ReLU(ReLU(X Wh + bh) Wo + bo)`. Recall that the ReLU function just sets every negative number in the matrix to zero. Also note that when you are adding a bias vector to a matrix, it is added to every single row in the matrix, which is called broadcasting.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda312c-b052-4e0f-b5d8-a1f5781b4b71",
   "metadata": {},
   "source": [
    "5. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, and which activation function should you use? What about for getting your network to predict housing prices,\n",
    "\n",
    "   ***To classify email into spam or ham, you just need one neuron in the output layer of a neural network—for example, indicating the probability that the email is spam. You would typically use the sigmoid activation function in the output layer when estimating a probability.***\n",
    "\n",
    "   ***If instead you want to tackle MNIST, you need 10 neurons in the output layer, and you must replace the sigmoid function with the softmax activation function, which can handle multiple classes, outputting one probability per class. If you want your neural network to predict housing prices like in Chapter 2, then you need one output neuron, using no activation function at all in the output layer. Note: when the values to predict can vary by many orders of magnitude, you may want to predict the logarithm of the target value rather than the target value directly. Simply computing the exponential of the neural network's output will give you the estimated value (since exp(log v) = v).***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b51a7-7d3f-468a-be7a-cd5b5ee161de",
   "metadata": {},
   "source": [
    "6. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "   \n",
    "   ***Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regard to every model parameter (all the weights and biases), then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thousands or millions of times, using many training batches, until the model parameters converge to values that (hopefully) minimize the cost function. To compute the gradients, backpropagation uses reverse-mode autodiff (although it wasn't called that when backpropagation was invented, and it has been reinvented several times). Reverse-mode autodiff performs a forward pass through a computation graph, computing every node's value for the current training batch, and then it performs a reverse pass, computing all the gradients at once (see Appendix B for more details). So what's the difference? Well, backpropagation refers to the whole process of training an artificial neural network using multiple backpropagation steps, each of which computes gradients and uses them to perform a Gradient Descent step. In contrast, reverse-mode autodiff is just a technique to compute gradients efficiently, and it happens to be used by backpropagation.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef7ffd-7d4c-41ed-9262-1f12a9c83127",
   "metadata": {},
   "source": [
    "7. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "\n",
    "   ***Here is a list of all the hyperparameters you can tweak in a basic MLP: the number of hidden layers, the number of neurons in each hidden layer, and the activation function used in each hidden layer and in the output layer. In general, the ReLU activation function (or one of its variants; see Chapter 11) is a good default for the hidden layers. For the output layer, in general you will want the sigmoid activation function for binary classification, the softmax activation function for multiclass classification, or no activation function for regression. If the MLP overfits the training data, you can try reducing the number of hidden layers and reducing the number of neurons per hidden layer.***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-neural",
   "language": "python",
   "name": "env-neural"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
