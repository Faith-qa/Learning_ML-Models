{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef86bf56-dc1f-494d-aaff-c2b3f8d7cc22",
   "metadata": {},
   "source": [
    "# Building Complex Models using function API\n",
    "\n",
    "The function api allows one to build neural networks with more complex topologies with multipla inputs or outputs\n",
    "\n",
    "***Nonsequential neural networks [wide & deep neural network]**\n",
    "\n",
    "this architecturw makes it possible for the neural network to learn both deep patterns using the deep path and simple rules through the short path. \n",
    "\n",
    "context:\n",
    "A regular MLP forces all the data to flow through the fullstack of layers, thus simple patterns in the data may end up being distorted by this sequence of transformations.let's test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a78bdbb-6903-438b-914c-0c62061f8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ca8956-9f47-42fa-a87a-3c13608289e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the data\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04015299-6314-4713-812c-bb963460e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "\n",
    "hidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "hidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "concat_layer = tf.keras.layers.Concatenate()\n",
    "output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "input_ = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
    "normalized = normalization_layer(input_)\n",
    "\n",
    "hidden1 = hidden_layer1(normalized)\n",
    "hidden2 = hidden_layer2(hidden1)\n",
    "concat = concat_layer([normalized, hidden2])\n",
    "output = output_layer(concat)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_], outputs=[output])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195dae7c-117b-4ed1-a201-84145821744b",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- step 1:- created 5 layers with the `Normalization layer` to standardize the inputs. {incase you are wondering, relu is an activation function}\n",
    "- step 2: created an `input object` i.e `_input`:- this is a specification of the kind of input the model will get includin it's `shape` and optionally its `dtype`. \n",
    "- step 3:- we call the `normalization layer` as a function and passing it the input object. it tells keras how yo connect the layers.\n",
    "- step 4: In the same way, we then pass `normalized` to `hidden_layer1`, which outputs `hidden1`, and we pass `hidden1` to `hidden_layer2`, which outputs `hidden2`.\n",
    "- step 5:- we use the `concat_layer` to concatenate the input and the second hidden layer’s output\n",
    "- step 6: pass  `concat` to the` output_layer`, which gives us the final output.\n",
    "- step 7:- create a Keras `Model`, specifying which inputs and outputs to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff4ff7-1de7-4005-b85d-1348f32a64e9",
   "metadata": {},
   "source": [
    "In an instance where we would want to send asubset of the features through the wide path, and a different subset through the deep path., we can use **multiple inputs**\n",
    "\n",
    "for example, suppose we want to send 5 features through the wide path, (feature 0 to 4) and 6 features throuh the deep path(feature 2 to 7), we can achieve this as follows;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7c6d8c-222c-41c1-abb5-6ad5e149490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_wide = tf.keras.layers.Input(shape=[5])\n",
    "input_deep = tf.keras.layers.Input(shape=[6])\n",
    "\n",
    "norm_layer_wide = tf.keras.layers.Normalization()\n",
    "norm_layer_deep = tf.keras.layers.Normalization()\n",
    "\n",
    "norm_wide = norm_layer_wide(input_wide)\n",
    "norm_deep = norm_layer_deep(input_deep)\n",
    "\n",
    "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33deb14b-798b-4905-acb8-ca95dfafe517",
   "metadata": {},
   "source": [
    "There are a few things to notice in the above code, compared to the previous one:\n",
    "\n",
    "- Each `dense` layer is created and called on the same line. strategy to make the code more concise\n",
    "- `tf.keras.layers.concatenate()` is used to create a `concatenate` layer and calls it with the given inputs.\n",
    "- `inputs=[input_wide, input_deep]` was specified when creating the model sice there were tow inputs.\n",
    "\n",
    "\n",
    "the model can be compiled as usual but when calling the `fit()` methof, instead of passing a single input matrix `X_train`, pair matrices must be passed `(X_train_wide, X_train_deep)` one per input.\n",
    " the same is true for `X_valid` and also for `X_test` and `X_new` when you call `evaluate()` or `predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6369da2-5054-490d-b40c-87c77219d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 442us/step - RootMeanSquaredError: 1.5601 - loss: 2.5308 - val_RootMeanSquaredError: 0.8676 - val_loss: 0.7527\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297us/step - RootMeanSquaredError: 0.7602 - loss: 0.5793 - val_RootMeanSquaredError: 0.9316 - val_loss: 0.8679\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289us/step - RootMeanSquaredError: 0.6746 - loss: 0.4556 - val_RootMeanSquaredError: 0.6486 - val_loss: 0.4207\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292us/step - RootMeanSquaredError: 0.6467 - loss: 0.4185 - val_RootMeanSquaredError: 0.6886 - val_loss: 0.4742\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289us/step - RootMeanSquaredError: 0.6316 - loss: 0.3992 - val_RootMeanSquaredError: 0.5901 - val_loss: 0.3483\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - RootMeanSquaredError: 0.6210 - loss: 0.3859 - val_RootMeanSquaredError: 0.8969 - val_loss: 0.8044\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - RootMeanSquaredError: 0.6138 - loss: 0.3769 - val_RootMeanSquaredError: 0.9047 - val_loss: 0.8184\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293us/step - RootMeanSquaredError: 0.6080 - loss: 0.3698 - val_RootMeanSquaredError: 1.3430 - val_loss: 1.8037\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - RootMeanSquaredError: 0.6037 - loss: 0.3647 - val_RootMeanSquaredError: 1.7235 - val_loss: 2.9706\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - RootMeanSquaredError: 0.6029 - loss: 0.3636 - val_RootMeanSquaredError: 1.3574 - val_loss: 1.8426\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297us/step - RootMeanSquaredError: 0.5965 - loss: 0.3560 - val_RootMeanSquaredError: 1.0329 - val_loss: 1.0668\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - RootMeanSquaredError: 0.5888 - loss: 0.3468 - val_RootMeanSquaredError: 0.6548 - val_loss: 0.4287\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - RootMeanSquaredError: 0.5892 - loss: 0.3472 - val_RootMeanSquaredError: 0.7003 - val_loss: 0.4905\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - RootMeanSquaredError: 0.5817 - loss: 0.3384 - val_RootMeanSquaredError: 0.5870 - val_loss: 0.3446\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - RootMeanSquaredError: 0.5786 - loss: 0.3348 - val_RootMeanSquaredError: 0.6639 - val_loss: 0.4407\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292us/step - RootMeanSquaredError: 0.5763 - loss: 0.3322 - val_RootMeanSquaredError: 0.6224 - val_loss: 0.3874\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - RootMeanSquaredError: 0.5770 - loss: 0.3330 - val_RootMeanSquaredError: 0.7792 - val_loss: 0.6071\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - RootMeanSquaredError: 0.5735 - loss: 0.3290 - val_RootMeanSquaredError: 0.8057 - val_loss: 0.6491\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - RootMeanSquaredError: 0.5720 - loss: 0.3273 - val_RootMeanSquaredError: 1.3372 - val_loss: 1.7882\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369us/step - RootMeanSquaredError: 0.5729 - loss: 0.3283 - val_RootMeanSquaredError: 1.2315 - val_loss: 1.5166\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step - RootMeanSquaredError: 0.5762 - loss: 0.3322\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    }
   ],
   "source": [
    "# initialize the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "X_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]\n",
    "\n",
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "\n",
    "# training the model\n",
    "history = model.fit((X_train_wide, X_train_deep), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_wide, X_valid_deep), y_valid))\n",
    "mse_test = model.evaluate((X_test_wide, X_test_deep), y_test)\n",
    "y_pred = model.predict((X_new_wide, X_new_deep))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1fe4b9-3b3e-4413-86ae-002bbe38a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-neural",
   "language": "python",
   "name": "env-neural"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
