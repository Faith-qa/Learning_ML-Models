{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b34ee4e-eeb4-4ba2-9376-a31863f30a3d",
   "metadata": {},
   "source": [
    "# Faster Optimizers\n",
    "given training DNN can be painfully slow, there are 4 ways to speedup training:\n",
    "\n",
    "- good activation function\n",
    "- using batch normalization\n",
    "- reusing parts of a pretrained network\n",
    "- using a faster optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a8ecc-5cf0-4a40-a6fd-f2ef50675738",
   "metadata": {},
   "source": [
    "## momentum\n",
    "momentum optimization:-  in contrast gradient descent will take small stepos when the slope is gentle and big steps when the slope is steep but it will never pick up speed.\n",
    "\n",
    "momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracys the local gradieny from the momentum vector and it updates the weights by adding the momentum vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ead809-7240-4f0f-a824-bae7c74db674",
   "metadata": {},
   "source": [
    "to implement momentum optimizer in keras, we use the `SGD` opitmizer and set its `momentum` hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb168e-1336-4376-afa6-3254383d8f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049570c7-42df-4e2a-bfdb-9661e41e092f",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient\n",
    "This measurs the gradient of the cost function not at the local position 0 but slightly ahead in the direction of the momentum.\n",
    "\n",
    "this works because momentum will always be pointing in the right direction towards the optimum\n",
    "\n",
    "To use the `NAG` we simply set the `nesterov=True` when creating the `SGD` optimizer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be7e68-1dac-4b6d-a664-30a28288823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,\n",
    "                                    nesterov=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d09b2-f5c7-44c7-a05e-21b7a4892786",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "\n",
    "Consider the elongated bowl problem again: gradient descent starts by quickly going down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down to the bottom of the valley. It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum. The AdaGrad algorithm‚Å†17 achieves this correction by scaling down the gradient vector along the steepest dimensions.\n",
    "\n",
    "in short the algorithm decays the learning rate faster for steep dimensions than for dimensions with gentler slopes.\n",
    "\n",
    "usually best for quadric problems but should not be used to train deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7fb540-2467-4f91-b476-5ea969561ab1",
   "metadata": {},
   "source": [
    "## RMSProps\n",
    "given AdaGrad runs the risk of slowing down abit too fast and never converging to the global optimium. the RMSProps fixes this by accumulating only the gradient from the most recent iterations, as opposed to all the gradients since the beginning of training.\n",
    "\n",
    "it does so by using the exponential decay in the fist step. \n",
    "\n",
    "this can be achieved by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f87cb-fd8f-49de-b33f-f36fe6e0d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df82526-5278-4ee0-a5ea-e41713bcc284",
   "metadata": {},
   "source": [
    "except on very simple problems, this optimizer almost always performs much better than Adagrad/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e031338-87d6-46e6-9ad4-6a71983d3ccc",
   "metadata": {},
   "source": [
    "## Adam (Adoptive momentum estimation)\n",
    "combines the idea of momentum optimization and the RSMprops.\n",
    "\n",
    "it keeps track of an exponentially decaying average past gradients. these are estimations of the mean and (uncentered) variance of the gradients. the mean is often called *first moment* while the variance is often called *the second moment* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-DNN",
   "language": "python",
   "name": "env-dnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
