{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c14c6e30-3c08-478c-82d7-2a615c9318d5",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks\n",
    "\n",
    "challenges with training deep neural networks:\n",
    "\n",
    "- gradients rowing smaller or larger during training. this makes lower layers very hard to train\n",
    "- small training data i.e too costly to labrl\n",
    "- a model with millions of parameters has a higher risk of over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c768d-dcc3-49c6-ad23-0ea13b6ccba5",
   "metadata": {},
   "source": [
    "## the Vanishing/Exploding Gradients problem\n",
    "\n",
    "gradients often get smaller and smaller as the algorithm progresses down to the lower layers. the gradient descent update leaves the lower layer's connection weights virtually unchanged and the training never converges to a good sollution. this is called the *vanishing gradients problem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e134f9-6898-47e7-bd85-649a571338ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dense = tf.keras.layers.Dense(50, activation=\"relu\", \n",
    "                              kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d7f70-327b-413b-b6bf-d678ddda9246",
   "metadata": {},
   "source": [
    "### better activation function\n",
    "\n",
    "problems with unstable gradients were partly due to a poor choice in activation functions. most people assumed that if mother nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice\n",
    "\n",
    "however it turns out there are betty activation functions that behave better in neural networks i.e `the ReLU activation function` because it does not saturate the positive values and it is fast to compute\n",
    "\n",
    "the **ReLU activation function is not perfect** it suffers from a problem known as the **the dying relu** where during training some neurons effectively 'die' i.e they stop outputting anything other than 0. this happens especially when one uses a large learning rate. \n",
    "\n",
    "A neuron dies when it's weights gets tweeked in such a way that the input of the ReLU function i.e the weighted sum of the neurons inputs plus it's bias term is negative for all instances in the training set.\n",
    "when this happens, it just keeps outputing zeross and gradient decent does not affect it anymore given it is zero when input is negative\n",
    "\n",
    "To fix this, one can use a variant of the ReLU function, i.e the leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1ae70-b546-4157-930c-0ef5d0d23b75",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "this techniquw consists of adding an operation in the model just before or after the activation function of each hidden layer. \n",
    "\n",
    "it simply zero centers and normalizes each input then scales and shifts the results using two new parameter vector layer: one for scaling, the other for shifting.  i.e the operation let's the model learn at optimal scale and mean of each of the layer's input. \n",
    "\n",
    "with it, there is no need to standardize the training set. i.e there is no need for `StandardScaler ` or `Normalization`: The BN layer will do it for you. \n",
    "\n",
    "to achieve this i.e zero-center and normalize the inputs, the algorithm needs to estimate each input's mean and standard deviation. it does so by evaluating the mean and standard deviation of the input over the current mini-batch(hence the name batch normalization)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-DNN",
   "language": "python",
   "name": "env-dnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
